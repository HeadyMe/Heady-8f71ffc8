<!--
  © 2026 Heady Systems LLC.
  PROPRIETARY AND CONFIDENTIAL.
  Unauthorized copying, modification, or distribution is strictly prohibited.
-->
# Competitive Battlecards (Per-Surface)

## How to Use These Battlecards

Each card covers: what they say → what we say → what we must prove → when we win / lose.

---

## Battlecard: HeadyAI-IDE vs GitHub Copilot / Cursor / Windsurf

### They Say

- "Fastest autocomplete, deeply integrated, massive training data"
- Copilot: "1B+ users, GitHub-native"
- Cursor: "AI-first IDE, agentic editing"
- Windsurf: "Cascade agents, flow state"

### We Say

- "Fast generation is table stakes. Trusted generation is the moat."
- "Arena Mode compares strategies before you ship — not after you break prod."
- "Consensus Engine: multi-model agreement reduces hallucination risk."
- "Every action logged, costed, and replayable."

### Must Prove

- Fix-loop time reduction vs baseline (before/after Arena)
- Rollback rate lower than competitors
- Cost per outcome visible (competitors hide this)
- Audit trail completeness = 100%

### We Win When

- Buyer cares about correctness + auditability
- Team has been burned by AI regressions
- Cost visibility matters (enterprise budgets)
- Security/compliance is a hard requirement

### We Lose When

- They only want fastest autocomplete
- No security owner or governance need
- Single-developer, no audit requirements

---

## Battlecard: HeadyMCP vs Native Agent Frameworks

### They Say

- "Our agent framework connects to everything"
- "Built-in tool use, function calling, plugins"

### We Say

- "Connect anything — but safely."
- "Permission manifests, audit trails, verified connectors."
- "Your agents run tools through a governed layer, not raw."

### Must Prove

- Integration time reduction
- Tool calls blocked by policy (show the value of governance)
- Connector quality scores (reliability, latency)

### We Win When

- Enterprise needs tool governance
- Team has had a tool-calling incident
- Audit requirements exist

### We Lose When

- "Move fast, break things" culture
- No compliance requirements
- Single-tool integrations only

---

## Battlecard: HeadyBuddy vs ChatGPT / Claude / Gemini Apps

### They Say

- "Best general AI assistant, massive knowledge"
- "Multimodal, fast, cheap"

### We Say

- "We don't just chat — we execute tasks with proof."
- "Cross-device continuity: start on desktop, finish on mobile."
- "Approval ladder: read → write → destructive, always transparent."
- "Your data stays sovereign — local-first option."

### Must Prove

- Tasks completed (not just messages sent)
- Cross-device handoff works reliably
- Privacy controls are real (local processing, data boundaries)

### We Win When

- User wants AI that DOES things (not just answers)
- Privacy/sovereignty matters
- Cross-device workflows are common
- They want to customize and control the assistant

### We Lose When

- They just want chat
- Brand loyalty to OpenAI/Anthropic/Google
- No task execution needs

---

## Battlecard: HeadyCheck vs Datadog / Grafana / New Relic

### They Say

- "Unified observability, APM, logs, traces, dashboards"
- "Industry standard, massive integrations"

### We Say

- "Ecosystem-native observability that reduces tool overload."
- "AI-driven remediation suggestions, not just dashboards."
- "Registry-driven: knows your services, health, costs, SLOs."
- "Integrated with the same platform that builds and ships."

### Must Prove

- MTTR reduction vs standalone observability
- Tool consolidation (fewer tools = lower cost)
- Remediation quality (suggestions that actually work)

### We Win When

- Tool sprawl is painful
- Budget pressure on observability spend
- Want observability + intelligence (not just data)

### We Lose When

- Deep APM/tracing needs (we're not there yet)
- Enterprise already locked into Datadog contracts
- Need petabyte-scale log analytics

---

## Battlecard: HeadyCloud vs Vercel / Netlify / Render / Fly.io

### They Say

- "Deploy in seconds, scale automatically"
- "Git-push deploys, edge functions, serverless"

### We Say

- "Deployment as governed extension of intelligence stack."
- "Deploy with Arena validation + audit proof."
- "Cost tracking per deployment, per environment."

### Must Prove

- Deploy reliability matches competitors
- Governance adds value without slowing down
- Cost transparency is superior

### We Win When

- Governance + audit requirements exist
- Want deployment integrated with AI workflow
- Cost visibility matters

### We Lose When

- Just want simplest possible deploy
- No governance needs
- Already happy with current platform
