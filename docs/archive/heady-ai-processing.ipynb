{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# üß† HeadyBuddy Brain ‚Äî A100 GPU Intelligence Engine\n",
        "\n",
        "This notebook runs HeadyBuddy's AI brain on Colab's A100 GPU.\n",
        "It serves a FastAPI chat endpoint that the HeadyBuddy widget calls.\n",
        "\n",
        "### Setup:\n",
        "1. **Runtime ‚Üí Change runtime type ‚Üí A100 GPU**\n",
        "2. Run all cells\n",
        "3. Copy the tunnel URL ‚Üí HeadyBuddy auto-connects\n",
        "\n",
        "### Architecture:\n",
        "```\n",
        "HeadyBuddy Widget (any site) ‚Üí API call ‚Üí This Colab (A100 GPU)\n",
        "                                              ‚Üì\n",
        "     User sees response      ‚Üê  LLM generates intelligent reply\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install"
      },
      "outputs": [],
      "source": [
        "# Cell 1: Install dependencies\n",
        "!pip install -q transformers accelerate torch flask flask-cors pyngrok\n",
        "!pip install -q bitsandbytes sentencepiece protobuf\n",
        "\n",
        "# Optional: set your ngrok auth token for persistent tunnel\n",
        "NGROK_AUTH_TOKEN = \"\"  # Paste your token here for stable URL\n",
        "\n",
        "print(\"‚úÖ Dependencies installed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_model"
      },
      "outputs": [],
      "source": [
        "# Cell 2: Load the LLM\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "MODEL_ID = \"microsoft/Phi-3.5-mini-instruct\"  # Fast, smart, fits A100\n",
        "\n",
        "print(f\"üîß Loading {MODEL_ID}...\")\n",
        "print(f\"üñ•Ô∏è GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n",
        "print(f\"üíæ VRAM: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB\" if torch.cuda.is_available() else \"\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# Create pipeline\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=512,\n",
        "    do_sample=True,\n",
        "    temperature=0.7,\n",
        "    top_p=0.9,\n",
        "    repetition_penalty=1.1\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ Model loaded on {model.device}\")\n",
        "print(f\"üìä Parameters: {sum(p.numel() for p in model.parameters()) / 1e9:.1f}B\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "system_prompt"
      },
      "outputs": [],
      "source": [
        "# Cell 3: HeadyBuddy System Prompt\n",
        "HEADY_SYSTEM_PROMPT = \"\"\"You are HeadyBuddy, the AI assistant for the Heady ecosystem. You are helpful, friendly, knowledgeable, and slightly cosmic in personality.\n",
        "\n",
        "The Heady ecosystem consists of 6 interconnected services built on Sacred Geometry principles:\n",
        "\n",
        "1. HeadySystems (headysystems.com) ‚Äî Infrastructure backbone, Metatron's Cube architecture, HCFP policy engine, system orchestration\n",
        "2. HeadyMe (headyme.com) ‚Äî Personal AI companion, Flower of Life pattern, preference management, privacy-first\n",
        "3. HeadyConnection (headyconnection.org) ‚Äî Social intelligence layer, Sri Yantra pattern, knowledge graph, collaboration\n",
        "4. HeadyIO (headyio.com) ‚Äî Data orchestration gateway, Torus pattern, API gateway, real-time streaming, webhooks\n",
        "5. HeadyBuddy (headybuddy.org) ‚Äî That's you! AI assistant, Seed of Life pattern, context-aware help on every site\n",
        "6. HeadyMCP (headymcp.com) ‚Äî Model Context Protocol hub, Vesica Piscis, 20+ AI tools for IDEs\n",
        "\n",
        "Key technologies:\n",
        "- HCFP (Heady Core Functionality Platform) ‚Äî auto-success policy engine, zero violations\n",
        "- HeadyBattle interceptor ‚Äî security engine\n",
        "- Cloudflare Workers + WARP tunnel ‚Äî zero-trust deployment\n",
        "- Sacred Geometry theming ‚Äî cosmic rainbow aesthetic across all sites\n",
        "- HeadyLens ‚Äî real-time system monitoring via WebSocket\n",
        "\n",
        "You can help with: system status, service details, architecture questions, troubleshooting, navigation between services, explaining features, and general AI assistance.\n",
        "\n",
        "Keep responses concise but thorough. Use emoji sparingly. Be warm and knowledgeable.\"\"\"\n",
        "\n",
        "def generate_response(user_message, context=None):\n",
        "    \"\"\"Generate an intelligent response using the LLM.\"\"\"\n",
        "    service_context = \"\"\n",
        "    if context and context.get('service'):\n",
        "        service_context = f\"\\nThe user is currently on: {context['service']}\"\n",
        "    \n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": HEADY_SYSTEM_PROMPT + service_context},\n",
        "        {\"role\": \"user\", \"content\": user_message}\n",
        "    ]\n",
        "    \n",
        "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    \n",
        "    result = pipe(prompt, return_full_text=False)\n",
        "    response = result[0]['generated_text'].strip()\n",
        "    \n",
        "    return response\n",
        "\n",
        "# Test it\n",
        "test = generate_response(\"What is the Heady ecosystem?\")\n",
        "print(f\"Test response:\\n{test}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "server"
      },
      "outputs": [],
      "source": [
        "# Cell 4: Start the API server\n",
        "import os, threading, time, json\n",
        "from flask import Flask, request, jsonify\n",
        "from flask_cors import CORS\n",
        "\n",
        "app = Flask(__name__)\n",
        "CORS(app)\n",
        "\n",
        "node_id = os.urandom(4).hex()\n",
        "gpu_name = torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\"\n",
        "start_time = time.time()\n",
        "request_count = 0\n",
        "\n",
        "@app.route('/health', methods=['GET'])\n",
        "def health():\n",
        "    return jsonify({\n",
        "        \"status\": \"OPTIMAL\",\n",
        "        \"service\": \"HeadyBuddy Brain\",\n",
        "        \"node_id\": node_id,\n",
        "        \"hardware\": gpu_name,\n",
        "        \"model\": MODEL_ID,\n",
        "        \"uptime\": round(time.time() - start_time, 1),\n",
        "        \"requests_served\": request_count\n",
        "    })\n",
        "\n",
        "@app.route('/api/chat', methods=['POST'])\n",
        "def chat():\n",
        "    global request_count\n",
        "    request_count += 1\n",
        "    \n",
        "    data = request.json or {}\n",
        "    message = data.get('message', '')\n",
        "    context = data.get('context', {})\n",
        "    \n",
        "    if not message:\n",
        "        return jsonify({\"error\": \"No message provided\"}), 400\n",
        "    \n",
        "    try:\n",
        "        response = generate_response(message, context)\n",
        "        return jsonify({\n",
        "            \"response\": response,\n",
        "            \"model\": MODEL_ID,\n",
        "            \"hardware\": gpu_name,\n",
        "            \"node_id\": node_id\n",
        "        })\n",
        "    except Exception as e:\n",
        "        return jsonify({\"error\": str(e)}), 500\n",
        "\n",
        "@app.route('/api/cloud-process', methods=['POST'])\n",
        "def process():\n",
        "    \"\"\"Legacy endpoint for backward compatibility.\"\"\"\n",
        "    data = request.json or {}\n",
        "    return jsonify({\n",
        "        \"status\": \"success\",\n",
        "        \"node_id\": node_id,\n",
        "        \"hardware\": gpu_name,\n",
        "        \"result\": f\"Processed {data.get('type', 'task')} on {gpu_name}\",\n",
        "        \"heady_optimized\": True\n",
        "    })\n",
        "\n",
        "# Start server\n",
        "server_thread = threading.Thread(target=lambda: app.run(host='0.0.0.0', port=5000, debug=False, use_reloader=False))\n",
        "server_thread.start()\n",
        "print(f\"\\nüöÄ HeadyBuddy Brain API running on port 5000\")\n",
        "print(f\"   GPU: {gpu_name}\")\n",
        "print(f\"   Model: {MODEL_ID}\")\n",
        "print(f\"   Endpoints: /health, /api/chat, /api/cloud-process\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tunnel"
      },
      "outputs": [],
      "source": [
        "# Cell 5: Create public tunnel\n",
        "import re, subprocess, time\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"üîó ESTABLISHING TUNNEL TO HEADY ECOSYSTEM...\")\n",
        "print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "public_url = None\n",
        "\n",
        "if NGROK_AUTH_TOKEN:\n",
        "    from pyngrok import ngrok, conf\n",
        "    conf.get_default().auth_token = NGROK_AUTH_TOKEN\n",
        "    public_url = ngrok.connect(5000).public_url\n",
        "    print(f\"‚úÖ NGROK TUNNEL: {public_url}\\n\")\n",
        "else:\n",
        "    print(\"Using Cloudflare Quick Tunnel (free, no auth needed)...\")\n",
        "    subprocess.run(['wget', '-q', 'https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb'], check=False)\n",
        "    subprocess.run(['dpkg', '-i', 'cloudflared-linux-amd64.deb'], check=False, capture_output=True)\n",
        "\n",
        "    p = subprocess.Popen(\n",
        "        ['cloudflared', 'tunnel', '--url', 'http://127.0.0.1:5000'],\n",
        "        stdout=subprocess.PIPE, stderr=subprocess.PIPE\n",
        "    )\n",
        "\n",
        "    import select\n",
        "    deadline = time.time() + 20\n",
        "    collected = \"\"\n",
        "    while time.time() < deadline:\n",
        "        ready, _, _ = select.select([p.stderr], [], [], 1)\n",
        "        if ready:\n",
        "            chunk = p.stderr.read1(4096).decode('utf-8', errors='replace')\n",
        "            collected += chunk\n",
        "            match = re.search(r'https://[a-zA-Z0-9-]+\\.trycloudflare\\.com', collected)\n",
        "            if match:\n",
        "                public_url = match.group(0)\n",
        "                break\n",
        "\n",
        "    if public_url:\n",
        "        print(f\"‚úÖ CLOUDFLARE TUNNEL: {public_url}\\n\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è Could not auto-detect URL. Check output:\\n\")\n",
        "        print(collected)\n",
        "\n",
        "if public_url:\n",
        "    print(\"=\"*60)\n",
        "    print(f\"\")\n",
        "    print(f\"  üß† HeadyBuddy Brain is LIVE!\")\n",
        "    print(f\"\")\n",
        "    print(f\"  üìã Your endpoint URL:\")\n",
        "    print(f\"  üëâ {public_url}\")\n",
        "    print(f\"\")\n",
        "    print(f\"  Test it:\")\n",
        "    print(f\"  curl {public_url}/health\")\n",
        "    print(f\"\")\n",
        "    print(f\"  Chat:\")\n",
        "    print(f\"  curl -X POST {public_url}/api/chat \\\\\")\n",
        "    print(f\"    -H 'Content-Type: application/json' \\\\\")\n",
        "    print(f\"    -d '{{\\\"message\\\": \\\"What is HeadyBuddy?\\\"}}'\")\n",
        "    print(f\"\")\n",
        "    print(f\"  To connect to HeadyBuddy widget:\")\n",
        "    print(f\"  Set BRAIN_URL in headybuddy-widget.js to:\")\n",
        "    print(f\"  {public_url}/api/chat\")\n",
        "    print(f\"\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "# Keep alive\n",
        "print(\"\\n‚è≥ Keeping notebook alive... (Ctrl+C to stop)\")\n",
        "while True:\n",
        "    time.sleep(60)\n",
        "    print(f\"üíì Heartbeat ‚Äî {request_count} requests served, uptime: {round(time.time() - start_time)}s\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}