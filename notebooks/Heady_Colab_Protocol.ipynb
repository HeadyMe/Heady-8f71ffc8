{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Heady Colab Protocol - GPU Optimized\n",
    "\n",
    "This notebook leverages Google Colab's GPU resources with **intelligent optimization** to run the Heady Admin Console and its AI capabilities efficiently.\n",
    "\n",
    "## üéØ Features\n",
    "- ‚úÖ **Automatic GPU Detection & Optimization**\n",
    "- ‚úÖ **Memory Management & Cleanup**\n",
    "- ‚úÖ **Real-time GPU Monitoring**\n",
    "- ‚úÖ **Smart Model Loading**\n",
    "- ‚úÖ **Ngrok Tunneling for Remote Access**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Step 1: Verify GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability and detailed info\n",
    "import torch\n",
    "import psutil\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"üîç Heady GPU Detection & Analysis\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"‚è∞ Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"üêç Python: {sys.version}\")\n",
    "print(f\"üî• PyTorch: {torch.__version__}\")\n",
    "print()\n",
    "\n",
    "# GPU Detection\n",
    "if torch.cuda.is_available():\n",
    "    device_count = torch.cuda.device_count()\n",
    "    current_device = torch.cuda.current_device()\n",
    "    \n",
    "    print(f\"‚úÖ GPU Available: {device_count} device(s)\")\n",
    "    print(f\"üéØ Current Device: {current_device}\")\n",
    "    print(f\"üìõ GPU Name: {torch.cuda.get_device_name(current_device)}\")\n",
    "    \n",
    "    # Memory info\n",
    "    total_memory = torch.cuda.get_device_properties(current_device).total_memory\n",
    "    allocated_memory = torch.cuda.memory_allocated(current_device)\n",
    "    cached_memory = torch.cuda.memory_reserved(current_device)\n",
    "    free_memory = total_memory - allocated_memory\n",
    "    \n",
    "    print(f\"üíæ Total Memory: {total_memory / 1024**3:.2f} GB\")\n",
    "    print(f\"üìä Allocated: {allocated_memory / 1024**3:.2f} GB\")\n",
    "    print(f\"üóÇÔ∏è Cached: {cached_memory / 1024**3:.2f} GB\")\n",
    "    print(f\"‚ú® Free: {free_memory / 1024**3:.2f} GB\")\n",
    "    print(f\"üìà Usage: {(allocated_memory / total_memory * 100):.1f}%\")\n",
    "    \n",
    "    # Try to get more detailed GPU info\n",
    "    try:\n",
    "        import pynvml\n",
    "        pynvml.nvmlInit()\n",
    "        handle = pynvml.nvmlDeviceGetHandleByIndex(current_device)\n",
    "        \n",
    "        # Temperature\n",
    "        temp = pynvml.nvmlDeviceGetTemperature(handle, pynvml.NVML_TEMPERATURE_GPU)\n",
    "        print(f\"üå°Ô∏è Temperature: {temp}¬∞C\")\n",
    "        \n",
    "        # Power usage\n",
    "        power = pynvml.nvmlDeviceGetPowerUsage(handle) / 1000.0\n",
    "        print(f\"‚ö° Power Usage: {power:.1f}W\")\n",
    "        \n",
    "        # Utilization\n",
    "        util = pynvml.nvmlDeviceGetUtilizationRates(handle)\n",
    "        print(f\"üìä GPU Utilization: {util.gpu}%\")\n",
    "        print(f\"üíæ Memory Utilization: {util.memory}%\")\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"üí° Install pynvml for detailed GPU metrics: !pip install pynvml\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Could not get detailed GPU info: {e}\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå GPU not available\")\n",
    "    print(\"üí° Enable GPU: Runtime > Change runtime type > GPU\")\n",
    "\n",
    "# System Memory\n",
    "system_memory = psutil.virtual_memory()\n",
    "print(f\"\\nüíª System Memory: {system_memory.total / 1024**3:.1f} GB\")\n",
    "print(f\"üìä System Memory Usage: {system_memory.percent:.1f}%\")\n",
    "print(f\"‚ú® Available: {system_memory.available / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Step 2: Install Dependencies & GPU Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages with GPU optimizations\n",
    "print(\"üì¶ Installing Heady dependencies...\")\n",
    "\n",
    "# Core dependencies\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 -q\n",
    "!pip install transformers accelerate psutil -q\n",
    "\n",
    "# Optional: Enhanced GPU monitoring\n",
    "!pip install pynvml -q\n",
    "\n",
    "# Heady dependencies\n",
    "!pip install flask flask-cors requests -q\n",
    "\n",
    "print(\"‚úÖ Dependencies installed successfully!\")\n",
    "\n",
    "# Clone Heady repository\n",
    "import os\n",
    "if not os.path.exists('HeadyMonorepo'):\n",
    "    !git clone https://github.com/HeadySystems/HeadyMonorepo.git\n",
    "    %cd HeadyMonorepo\n",
    "else:\n",
    "    %cd HeadyMonorepo\n",
    "    !git pull\n",
    "\n",
    "print(\"üìÅ Heady repository ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Step 3: Initialize GPU Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the GPU optimizer\n",
    "import sys\n",
    "sys.path.append('backend/python_worker')\n",
    "\n",
    "from heady_project.gpu_optimizer import gpu_optimizer\n",
    "import time\n",
    "\n",
    "print(\"üöÄ Initializing Heady GPU Optimizer...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Get initial GPU status\n",
    "gpu_optimizer.print_gpu_status()\n",
    "\n",
    "# Get efficiency score\n",
    "efficiency = gpu_optimizer.get_memory_efficiency_score()\n",
    "print(f\"\\nüìà Initial Efficiency Score: {efficiency['overall_score']:.1f}/100\")\n",
    "\n",
    "# Get suggestions\n",
    "suggestions = gpu_optimizer.suggest_optimizations()\n",
    "if suggestions:\n",
    "    print(f\"\\nüí° {len(suggestions)} Optimization Suggestions:\")\n",
    "    for i, suggestion in enumerate(suggestions, 1):\n",
    "        print(f\"   {i}. {suggestion}\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ GPU configuration looks good!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Step 4: Setup Ngrok Tunnel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install and setup ngrok for tunneling\n",
    "!pip install pyngrok -q\n",
    "\n",
    "from pyngrok import ngrok, conf\n",
    "import getpass\n",
    "\n",
    "print(\"üåê Setting up Ngrok tunnel...\")\n",
    "\n",
    "# Get ngrok authtoken (secure input)\n",
    "authtoken = getpass.getpass(\"Enter your Ngrok authtoken (from https://dashboard.ngrok.com/auth): \")\n",
    "\n",
    "# Configure ngrok\n",
    "conf.get_default().auth_token = authtoken\n",
    "\n",
    "# Start ngrok tunnel\n",
    "public_url = ngrok.connect(3300).public_url\n",
    "print(f\"\\n‚úÖ Ngrok tunnel established!\")\n",
    "print(f\"üîó Public URL: {public_url}\")\n",
    "print(f\"üîí This URL will give you access to the Heady Admin Console\")\n",
    "\n",
    "# Store URL for later use\n",
    "%env PUBLIC_URL={public_url}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Step 5: Start Heady Admin Console"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the Heady Admin Console with GPU optimization\n",
    "import os\n",
    "import subprocess\n",
    "import threading\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Set environment variables\n",
    "os.environ['HEADY_API_KEY'] = 'colab_secure_token'\n",
    "os.environ['GPU_MEMORY_LIMIT'] = '10240'  # 10GB limit\n",
    "\n",
    "print(\"üöÄ Starting Heady Admin Console...\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"‚è∞ Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"üîó Access URL: {os.getenv('PUBLIC_URL')}\")\n",
    "print(f\"üîë Admin Token: colab_secure_token\")\n",
    "print()\n",
    "\n",
    "# Optimize GPU before starting\n",
    "print(\"üßπ Optimizing GPU memory before startup...\")\n",
    "result = gpu_optimizer.optimize_gpu_memory(aggressive=True)\n",
    "print(f\"‚úÖ {result.message}\")\n",
    "\n",
    "# Start the server\n",
    "def start_server():\n",
    "    try:\n",
    "        # Change to backend directory\n",
    "        os.chdir('backend')\n",
    "        \n",
    "        # Start the Node.js server\n",
    "        process = subprocess.Popen(\n",
    "            ['node', 'index.js'],\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.STDOUT,\n",
    "            universal_newlines=True\n",
    "        )\n",
    "        \n",
    "        # Print output in real-time\n",
    "        for line in iter(process.stdout.readline, ''):\n",
    "            if line.strip():\n",
    "                print(f\"üì° {line.strip()}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Server error: {e}\")\n",
    "\n",
    "# Start server in background thread\n",
    "server_thread = threading.Thread(target=start_server, daemon=True)\n",
    "server_thread.start()\n",
    "\n",
    "# Give server time to start\n",
    "time.sleep(5)\n",
    "\n",
    "print(\"\\n‚úÖ Heady Admin Console is starting up!\")\n",
    "print(f\"üåê Access it at: {os.getenv('PUBLIC_URL')}\")\n",
    "print(\"üîë Use token: colab_secure_token\")\n",
    "print(\"\\n‚è≥ Waiting for server to fully initialize...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Step 6: Real-time GPU Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor GPU usage in real-time\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as display\n",
    "import time\n",
    "\n",
    "print(\"üìä Starting real-time GPU monitoring...\")\n",
    "print(\"üîÑ This will update every 10 seconds for 2 minutes\")\n",
    "print(\"‚èπÔ∏è Interrupt the cell to stop monitoring\")\n",
    "\n",
    "# Monitor for 2 minutes (12 intervals of 10 seconds)\n",
    "monitoring_data = []\n",
    "\n",
    "try:\n",
    "    for i in range(12):\n",
    "        # Clear previous output\n",
    "        display.clear_output(wait=True)\n",
    "        \n",
    "        # Get current metrics\n",
    "        metrics = gpu_optimizer.get_gpu_metrics()\n",
    "        efficiency = gpu_optimizer.get_memory_efficiency_score()\n",
    "        \n",
    "        # Store data\n",
    "        data_point = {\n",
    "            'time': datetime.now().strftime('%H:%M:%S'),\n",
    "            'gpu_memory_gb': metrics.allocated_memory_gb,\n",
    "            'gpu_utilization': metrics.utilization_percent,\n",
    "            'efficiency_score': efficiency['overall_score'],\n",
    "            'temperature': metrics.temperature_celsius\n",
    "        }\n",
    "        monitoring_data.append(data_point)\n",
    "        \n",
    "        # Display current status\n",
    "        print(f\"üìä GPU Monitor - Update {i+1}/12\")\n",
    "        print(\"=\" * 40)\n",
    "        print(f\"‚è∞ Time: {data_point['time']}\")\n",
    "        print(f\"üíæ GPU Memory: {data_point['gpu_memory_gb']:.2f} GB\")\n",
    "        print(f\"üìä GPU Utilization: {data_point['gpu_utilization']:.1f}%\")\n",
    "        print(f\"üìà Efficiency Score: {data_point['efficiency_score']:.1f}/100\")\n",
    "        if data_point['temperature'] > 0:\n",
    "            print(f\"üå°Ô∏è Temperature: {data_point['temperature']:.1f}¬∞C\")\n",
    "        \n",
    "        # Auto-optimize if needed\n",
    "        optimization_result = gpu_optimizer.auto_optimize_if_needed(memory_threshold_percent=85.0)\n",
    "        if optimization_result and optimization_result.success:\n",
    "            print(f\"üßπ Auto-optimized: {optimization_result.message}\")\n",
    "        \n",
    "        # Show suggestions if efficiency is low\n",
    "        if efficiency['overall_score'] < 70:\n",
    "            suggestions = gpu_optimizer.suggest_optimizations()[:3]  # Top 3\n",
    "            if suggestions:\n",
    "                print(\"\\nüí° Quick Suggestions:\")\n",
    "                for suggestion in suggestions:\n",
    "                    print(f\"   ‚Ä¢ {suggestion}\")\n",
    "        \n",
    "        print(\"\\n‚è≥ Next update in 10 seconds...\")\n",
    "        \n",
    "        # Wait for next update\n",
    "        time.sleep(10)\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n‚èπÔ∏è Monitoring stopped by user\")\n",
    "\n",
    "print(f\"\\n‚úÖ Monitoring complete! Collected {len(monitoring_data)} data points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßπ Step 7: Manual GPU Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual GPU optimization controls\n",
    "print(\"üßπ Heady GPU Optimization Tools\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Show current status\n",
    "print(\"\\nüìä Current GPU Status:\")\n",
    "gpu_optimizer.print_gpu_status()\n",
    "\n",
    "# Optimization options\n",
    "print(\"\\nüîß Optimization Options:\")\n",
    "print(\"1. Light optimization (fast, minimal cleanup)\")\n",
    "print(\"2. Aggressive optimization (slower, thorough cleanup)\")\n",
    "print(\"3. Monitor GPU for 30 seconds\")\n",
    "\n",
    "# Get user choice\n",
    "choice = input(\"\\nSelect option (1/2/3): \").strip()\n",
    "\n",
    "if choice == \"1\":\n",
    "    print(\"\\nüßπ Running light optimization...\")\n",
    "    result = gpu_optimizer.optimize_gpu_memory(aggressive=False)\n",
    "    print(f\"‚úÖ {result.message}\")\n",
    "    \n",
    "elif choice == \"2\":\n",
    "    print(\"\\nüßπ Running aggressive optimization...\")\n",
    "    result = gpu_optimizer.optimize_gpu_memory(aggressive=True)\n",
    "    print(f\"‚úÖ {result.message}\")\n",
    "    \n",
    "elif choice == \"3\":\n",
    "    print(\"\\nüìä Monitoring GPU for 30 seconds...\")\n",
    "    data = gpu_optimizer.monitor_gpu_usage(duration_seconds=30, interval_seconds=5)\n",
    "    \n",
    "    if data:\n",
    "        print(\"\\nüìà Monitoring Results:\")\n",
    "        for i, point in enumerate(data):\n",
    "            print(f\"   {i+1}. Memory: {point['gpu_memory_allocated_gb']:.2f}GB, \"\n",
    "                  f\"Utilization: {point['gpu_utilization_percent']:.1f}%\")\n",
    "    else:\n",
    "        print(\"‚ùå No GPU available for monitoring\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå Invalid choice\")\n",
    "\n",
    "# Show final status\n",
    "print(\"\\nüìä Final GPU Status:\")\n",
    "gpu_optimizer.print_gpu_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Step 8: Test NLP Services with GPU\n"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test NLP services to verify GPU usage\n",
    "import sys\n",
    "sys.path.append('backend/python_worker')\n",
    "\n",
    "from heady_project.nlp_service import nlp_service\n",
    "import time\n",
    "\n",
    "print(\"üß† Testing NLP Services with GPU\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Test text\n",
    "test_text = \"\"\"\n",
    "The Heady Systems ecosystem represents a comprehensive approach to AI-driven infrastructure management. \n",
    "It integrates multiple specialized nodes including LENS for monitoring, MEMORY for persistent storage, \n",
    "BRAIN for intelligent processing, and CONDUCTOR for orchestration. This sacred geometry approach \n",
    "ensures optimal resource utilization and system harmony.\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nüìù Testing Text Summarization...\")\n",
    "start_time = time.time()\n",
    "summary = nlp_service.summarize_text(test_text, max_length=50, min_length=20)\n",
    "summarization_time = time.time() - start_time\n",
    "\n",
    "print(f\"‚úÖ Summary: {summary}\")\n",
    "print(f\"‚è±Ô∏è Time taken: {summarization_time:.2f} seconds\")\n",
    "\n",
    "# Get GPU status after summarization\n",
    "gpu_status = nlp_service.get_gpu_status()\n",
    "print(f\"\\nüìä GPU Status after summarization:\")\n",
    "print(f\"   Available: {gpu_status['gpu_available']}\")\n",
    "print(f\"   Memory Used: {gpu_status['allocated_memory_gb']:.2f}GB\")\n",
    "print(f\"   Efficiency: {gpu_status['efficiency_score']:.1f}/100\")\n",
    "\n",
    "print(\"\\nüí¨ Testing Text Generation...\")\n",
    "start_time = time.time()\n",
    "prompt = \"The future of AI infrastructure is\"\n",
    "response = nlp_service.generate_response(prompt, max_length=50)\n",
    "generation_time = time.time() - start_time\n",
    "\n",
    "print(f\"‚úÖ Generated: {response}\")\n",
    "print(f\"‚è±Ô∏è Time taken: {generation_time:.2f} seconds\")\n",
    "\n",
    "# Final GPU status\n",
    "final_status = nlp_service.get_gpu_status()\n",
    "print(f\"\\nüìä Final GPU Status:\")\n",
    "print(f\"   Memory Used: {final_status['allocated_memory_gb']:.2f}GB\")\n",
    "print(f\"   Efficiency: {final_status['efficiency_score']:.1f}/100\")\n",
    "\n",
    "if final_status['suggestions']:\n",
    "    print(f\"\\nüí° Suggestions:\")\n",
    "    for suggestion in final_status['suggestions'][:3]:\n",
    "        print(f\"   ‚Ä¢ {suggestion}\")\n",
    "\n",
    "print(\"\\n‚úÖ NLP services test complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Summary & Access Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary and access information\n",
    "print(\"üéâ Heady Colab Setup Complete!\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Access information\n",
    "public_url = os.getenv('PUBLIC_URL', 'Not set')\n",
    "print(f\"\\nüåê Admin Console URL: {public_url}\")\n",
    "print(f\"üîë Admin Token: colab_secure_token\")\n",
    "\n",
    "# Final GPU status\n",
    "print(f\"\\nüìä Final GPU Status:\")\n",
    "final_metrics = gpu_optimizer.get_gpu_metrics()\n",
    "final_efficiency = gpu_optimizer.get_memory_efficiency_score()\n",
    "\n",
    "if final_metrics.gpu_available:\n",
    "    print(f\"   GPU: {final_metrics.gpu_name}\")\n",
    "    print(f\"   Memory Used: {final_metrics.allocated_memory_gb:.2f}GB / {final_metrics.total_memory_gb:.2f}GB\")\n",
    "    print(f\"   Efficiency Score: {final_efficiency['overall_score']:.1f}/100\")\n",
    "    print(f\"   Utilization: {final_metrics.utilization_percent:.1f}%\")\nelse:\n",
    "    print(\"   GPU: Not available\")\n",
    "\n",
    "# Optimization history\n",
    "print(f\"\\nüßπ Optimization History: {len(gpu_optimizer.optimization_history)} optimizations performed\")\n",
    "for i, opt in enumerate(gpu_optimizer.optimization_history[-3:], 1):  # Last 3\n",
    "    print(f\"   {i}. {opt.message}\")\n",
    "\n",
    "print(f\"\\nüí° Pro Tips:\")\n",
    "print(\"   ‚Ä¢ Run the optimization cell periodically if GPU memory gets high\")\n",
    "print(\"   ‚Ä¢ Monitor the efficiency score - keep it above 70 for best performance\")\n",
    "print(\"   ‚Ä¢ The system auto-optimizes, but manual optimization can help\")\n",
    "print(\"   ‚Ä¢ Check temperature - if >80¬∞C, consider reducing workload\")\n",
    "\n",
    "print(f\"\\n‚ú® Your Heady Admin Console is ready at: {public_url}\")\n",
    "print(\"üîë Use the token: colab_secure_token\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
