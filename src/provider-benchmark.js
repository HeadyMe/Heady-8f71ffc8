/*
 * © 2026 Heady Systems LLC.
 * PROPRIETARY AND CONFIDENTIAL.
 * Unauthorized copying, modification, or distribution is strictly prohibited.
 */
/**
 * ─── Heady Provider Benchmark ─────────────────────────────────────
 * Measures connection speed, latency, and throughput for each remote
 * provider. Results auto-tune the routing table in the orchestrator.
 *
 * Connection Types Tested:
 *   REST (standard)     — fetch + JSON parse
 *   REST (streaming)    — SSE / chunked transfer
 *   WebSocket           — persistent bidirectional
 *   SDK Direct          — native SDK calls (HF, HeadyPythia, HeadyNexus)
 *
 * Benchmark stores results in vector memory for pattern learning.
 * ──────────────────────────────────────────────────────────────────
 */

const fs = require("fs");
const path = require("path");
const http = require("http");
const https = require("https");

const BENCH_FILE = path.join(__dirname, "..", "data", "provider-benchmarks.json");
const BENCH_AUDIT = path.join(__dirname, "..", "data", "benchmark-audit.jsonl");
const dir = path.dirname(BENCH_FILE);
if (!fs.existsSync(dir)) fs.mkdirSync(dir, { recursive: true });

// Persistent benchmark results
let benchResults = {};
try { benchResults = JSON.parse(fs.readFileSync(BENCH_FILE, "utf-8")); } catch { }

function saveBenchmarks() {
    try { fs.writeFileSync(BENCH_FILE, JSON.stringify(benchResults, null, 2)); } catch { }
}
function audit(entry) {
    try { fs.appendFileSync(BENCH_AUDIT, JSON.stringify({ ...entry, ts: new Date().toISOString() }) + "\n"); } catch { }
}

// ── Latency Test (HTTP ping) ────────────────────────────────────
function httpPing(url, timeout = 10000) {
    return new Promise((resolve) => {
        const start = Date.now();
        const mod = url.startsWith("https") ? https : http;
        const req = mod.get(url, { timeout }, (res) => {
            res.on("data", () => { });
            res.on("end", () => resolve({ latency: Date.now() - start, status: res.statusCode, ok: true }));
        });
        req.on("error", (err) => resolve({ latency: Date.now() - start, ok: false, error: err.message }));
        req.on("timeout", () => { req.destroy(); resolve({ latency: timeout, ok: false, error: "timeout" }); });
    });
}

// ── SDK Call Benchmark ──────────────────────────────────────────
async function benchmarkHF() {
    const start = Date.now();
    try {
        const { InferenceClient } = require("@headyhub/inference");
        const tokens = [process.env.HF_TOKEN, process.env.HF_TOKEN_2, process.env.HF_TOKEN_3].filter(Boolean);
        if (tokens.length === 0) return { provider: "hf", ok: false, error: "no tokens" };

        const client = new InferenceClient(tokens[0]);
        const sdkInit = Date.now() - start;

        const embedStart = Date.now();
        await client.featureExtraction({ model: "sentence-transformers/all-MiniLM-L6-v2", inputs: "benchmark test" });
        const embedLatency = Date.now() - embedStart;

        const chatStart = Date.now();
        await client.chatCompletion({
            model: "Qwen/Qwen3-235B-A22B",
            messages: [{ role: "user", content: "Say 'benchmark ok' in 5 words or less" }],
            max_tokens: 20,
        });
        const chatLatency = Date.now() - chatStart;

        return {
            provider: "hf", ok: true, sdkInit, embedLatency, chatLatency,
            totalLatency: Date.now() - start, tokens: tokens.length,
            connectionType: "REST (SDK)", protocol: "HTTPS",
        };
    } catch (err) {
        return { provider: "hf", ok: false, error: err.message, totalLatency: Date.now() - start };
    }
}

async function benchmarkHeadyPythia() {
    const start = Date.now();
    try {
        const { GoogleGenAI } = require("@google/genai");
        const keys = [process.env.GOOGLE_API_KEY, process.env.HEADY_PYTHIA_KEY_HEADY, process.env.HEADY_PYTHIA_KEY_GCLOUD].filter(Boolean);
        if (keys.length === 0) return { provider: "headypythia", ok: false, error: "no keys" };

        const ai = new GoogleGenAI({ apiKey: keys[0] });
        const sdkInit = Date.now() - start;

        const chatStart = Date.now();
        await ai.models.generateContent({ model: "headypythia-2.5-flash", contents: "Say 'benchmark ok' in 5 words or less" });
        const chatLatency = Date.now() - chatStart;

        return {
            provider: "headypythia", ok: true, sdkInit, chatLatency,
            totalLatency: Date.now() - start, keys: keys.length,
            connectionType: "REST (SDK)", protocol: "HTTPS",
        };
    } catch (err) {
        return { provider: "headypythia", ok: false, error: err.message, totalLatency: Date.now() - start };
    }
}

async function benchmarkHeadyJules() {
    const start = Date.now();
    try {
        const HeadyNexus = require("@headynexus-ai/sdk");
        const keys = [process.env.HEADY_NEXUS_KEY, process.env.HEADY_NEXUS_KEY_SECONDARY].filter(Boolean);
        if (keys.length === 0) return { provider: "headyjules", ok: false, error: "no keys" };

        const client = new HeadyNexus({ apiKey: keys[0] });
        const sdkInit = Date.now() - start;

        const chatStart = Date.now();
        await client.messages.create({
            model: "headyjules-sonnet-4-20250514", max_tokens: 20,
            messages: [{ role: "user", content: "Say 'benchmark ok' in 5 words or less" }],
        });
        const chatLatency = Date.now() - chatStart;

        return {
            provider: "headyjules", ok: true, sdkInit, chatLatency,
            totalLatency: Date.now() - start, keys: keys.length,
            connectionType: "REST (SDK)", protocol: "HTTPS",
        };
    } catch (err) {
        return { provider: "headyjules", ok: false, error: err.message, totalLatency: Date.now() - start };
    }
}

async function benchmarkLocal() {
    const start = Date.now();
    const ping = await httpPing("https://127.0.0.1:3301/api/pulse");
    return {
        provider: "local-manager", ok: ping.ok, pingLatency: ping.latency,
        totalLatency: Date.now() - start,
        connectionType: "HTTP (local)", protocol: "HTTP",
    };
}

async function benchmarkEdge() {
    const start = Date.now();
    const ping = await httpPing("https://headysystems.com/api/health", 8000);
    return {
        provider: "cloudflare-edge", ok: ping.ok, pingLatency: ping.latency,
        totalLatency: Date.now() - start,
        connectionType: "HTTPS (edge CDN)", protocol: "HTTPS + TLS 1.3",
    };
}

// ── Full Benchmark Suite ────────────────────────────────────────
async function runFullBenchmark(vectorMem) {
    const benchStart = Date.now();

    const results = await Promise.allSettled([
        benchmarkLocal(),
        benchmarkEdge(),
        benchmarkHF(),
        benchmarkHeadyPythia(),
        benchmarkHeadyJules(),
    ]);

    const benchmarks = results.map(r => r.status === "fulfilled" ? r.value : { ok: false, error: r.reason?.message });

    // Sort by total latency (fastest first)
    const sorted = benchmarks.filter(b => b.ok).sort((a, b) => (a.totalLatency || 999) - (b.totalLatency || 999));

    const report = {
        timestamp: new Date().toISOString(),
        totalDuration: Date.now() - benchStart,
        results: benchmarks,
        ranking: sorted.map((b, i) => ({
            rank: i + 1,
            provider: b.provider,
            totalLatency: b.totalLatency,
            connectionType: b.connectionType,
            protocol: b.protocol,
        })),
        fastest: sorted[0]?.provider || "none",
        recommended: {
            embedding: "hf",     // HF has best embedding support
            reasoning: sorted.find(b => ["headyjules", "headypythia"].includes(b.provider))?.provider || "headypythia",
            fast_chat: sorted.find(b => b.provider !== "local-manager")?.provider || "headypythia",
            edge_cache: "cloudflare-edge",
        },
    };

    // Store in persistent benchmarks
    benchResults = { ...benchResults, latest: report, history: [...(benchResults.history || []).slice(-50), report] };
    saveBenchmarks();
    audit({ type: "benchmark:complete", fastest: report.fastest, duration: report.totalDuration, providers: benchmarks.length });

    // Store in vector memory for pattern learning
    if (vectorMem && typeof vectorMem.ingestMemory === "function") {
        await vectorMem.ingestMemory({
            content: `Provider benchmark: fastest=${report.fastest} (${sorted[0]?.totalLatency}ms). Ranking: ${report.ranking.map(r => `${r.rank}.${r.provider}(${r.totalLatency}ms)`).join(", ")}`,
            metadata: { type: "benchmark", fastest: report.fastest, ts: report.timestamp },
        }).catch(() => { });
    }

    return report;
}

// ── Express Routes ──────────────────────────────────────────────
function registerRoutes(app, vectorMem) {
    app.get("/api/benchmark/latest", (req, res) => {
        res.json({ ok: true, ...(benchResults.latest || { message: "No benchmarks yet. POST /api/benchmark/run to start." }) });
    });

    app.post("/api/benchmark/run", async (req, res) => {
        try {
            const report = await runFullBenchmark(vectorMem);
            res.json({ ok: true, ...report });
        } catch (err) {
            res.status(500).json({ error: err.message });
        }
    });

    app.get("/api/benchmark/history", (req, res) => {
        res.json({ ok: true, history: benchResults.history || [], total: (benchResults.history || []).length });
    });

    console.log("  ∞ ProviderBenchmark: LOADED (GET /api/benchmark/latest, POST /run, GET /history)");
}

module.exports = { runFullBenchmark, registerRoutes, benchResults };
